{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f79c320-defb-4869-ac2a-83c98e9b6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# automatically reload the .py files being used for event selections and signal regions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21d1701-fa75-468e-8795-d6f1e2f8dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (25.0.1)\n",
      "Requirement already satisfied: uproot in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.6.0)\n",
      "Requirement already satisfied: awkward in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.8.1)\n",
      "Requirement already satisfied: vector in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.1)\n",
      "Requirement already satisfied: cramjam>=2.5.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uproot) (2.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uproot) (2025.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uproot) (2.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uproot) (23.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uproot) (3.4.1)\n",
      "Requirement already satisfied: awkward-cpp==45 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from awkward) (45)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from awkward) (7.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from importlib-metadata>=4.13.0->awkward) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys # import\n",
    "# # update the pip package installer\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "# # install required packages\n",
    "!{sys.executable} -m pip install --upgrade --user uproot awkward vector matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21753612-89d1-4086-9b19-b838e242a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.0.2 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.11.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp) (1.19.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\denai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --user numpy==2.0.2\n",
    "!{sys.executable} -m pip install --user requests\n",
    "!{sys.executable} -m pip install --user aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e388873-dc8f-4c20-96dd-9f4bc41bfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys #reimport sys so we have it when not running package installation/setup\n",
    "import infofile # local file containing cross-sections, sums of weights, dataset IDs\n",
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib_inline # to edit the inline plot format\n",
    "\n",
    "#suspicious this is necessary, never had to do it before!\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg') # to make plots in pdf (vector) format\n",
    "\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "import uproot # for reading .root files\n",
    "import awkward as ak # to represent nested data in columnar format\n",
    "import vector # for 4-momentum calculations\n",
    "import time\n",
    "MeV = 0.001\n",
    "GeV = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc3945d1-d6ef-491a-8731-513bc3ff56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import sys\n",
    "sys.path.append('.')  # Ensure current directory is in path\n",
    "\n",
    "# import signal regions and preselection\n",
    "# all of these take (data, channel)\n",
    "from region_functions import (\n",
    "    apply_preselection,\n",
    "    signal_region,\n",
    "    signal_region_VBF_new\n",
    ")\n",
    "\n",
    "# import specific functions\n",
    "from selection_functions import (\n",
    "    lep_type_selection,\n",
    "    lep_charge_selection,\n",
    "    lep_trigger_selection,\n",
    "    lep_pt_selection,\n",
    "    tight_selection,\n",
    "    met_selection,\n",
    "    calc_phi_dilepton,\n",
    "    lepton_mll_selection,\n",
    "    ptmiss_selection,\n",
    "    calc_mass,\n",
    "    calc_mT,\n",
    "    calc_mt_ell,\n",
    "    calc_mjj,\n",
    "    calc_pT_ll,\n",
    "    delta_phi,\n",
    "    central_jet_veto,\n",
    "    outside_lepton_veto,\n",
    "    outside_lepton_veto,\n",
    "    lepton_eta_selection,\n",
    "    remove_jets_near_leptons,\n",
    "    calc_rapidity,\n",
    "    delta_y_jj,\n",
    "    compute_pT_ll\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39576f22-ee25-4de5-8182-b7556398a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import os # for files\n",
    "from uproot import open as uproot_open # for files\n",
    "import json # for saving files\n",
    "import gc # for memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f2040f-4da8-41ba-afbd-517983ca8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/\" # file path\n",
    "skims = ['4lep','3lep','2lep','1lep','1largeRjet1lep','1lep1tau','GamGam'] # will use 2lep for my analysis\n",
    "\n",
    "# variables used \n",
    "variables = ['trigE', 'trigM', 'lep_pt', 'lep_phi', 'lep_type', 'lep_charge',\n",
    "            'met_et', 'met_phi', 'jet_n', 'jet_MV2c10', 'jet_pt', 'jet_eta', 'jet_phi', 'jet_E',\n",
    "             'lep_isTightID', 'lep_eta', 'lep_E', 'ditau_m',\n",
    "            ]\n",
    "# skim for 2 leptons\n",
    "skim_2lep = skims[2] \n",
    "\n",
    "lumi = 10  # fb^-1 (total luminosity for ABCD combined)\n",
    "\n",
    "# weight variables\n",
    "weight_vars = [\"mcWeight\", \"scaleFactor_PILEUP\", \"scaleFactor_ELE\", \"scaleFactor_MUON\", \"scaleFactor_LepTRIGGER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c92668-7ea6-44f7-9c0f-7104408ca231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC samples used\n",
    "samples = {\n",
    "    'Zjets': {\n",
    "        'list': ['Zee', 'Zmumu', 'Ztautau'],\n",
    "        'color': \"#8fd7d7\",\n",
    "        'legend': r'$Z+$jets'\n",
    "    },\n",
    "    'Wjets': {\n",
    "        'list': ['Wplusenu', 'Wminusenu', 'Wplusmunu', 'Wminusmunu', 'Wplustaunu', 'Wminustaunu'],\n",
    "        'color': \"#00b0be\",\n",
    "        'legend': r'$W+$jets'\n",
    "    },\n",
    "    'diboson_leptonic': {\n",
    "        'list': ['llll', 'lllv', 'llvv', 'lvvv'],\n",
    "        'color': '#ff8c9a',\n",
    "        'legend': 'Diboson (fully leptonic)'\n",
    "    },\n",
    "    'diboson_semileptonic': {\n",
    "        'list': ['ZqqZll', 'WqqZll', 'WpqqWmlv', 'WplvWmqq', 'WlvZqq'],\n",
    "        'color': \"#f45f74\",\n",
    "        'legend': 'Diboson (semi-leptonic)'\n",
    "    },\n",
    "    'HWW': {\n",
    "        'list': ['VBFH125_WW2lep', 'ggH125_WW2lep'],\n",
    "        'color': \"#ffcd8e\",\n",
    "        'legend': r'$H(125)\\rightarrow WW \\rightarrow lvlv$',\n",
    "    },\n",
    "    'top': {\n",
    "        'list': ['ttbar_lep', 'single_top_tchan', 'single_antitop_tchan', 'single_top_wtchan', 'single_antitop_wtchan',\n",
    "                 'single_top_schan', 'single_antitop_schan', 'ttW', 'ttee', 'ttmumu'],\n",
    "        'color': \"#ffb255\",\n",
    "        'legend': r'Top processes',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe739c69-b64d-4541-b18f-ed592b835f33",
   "metadata": {},
   "source": [
    "# running on MC samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb9e9886-6f46-4da4-b7ca-9c75b4101b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Channel: 0ggF\n",
      "\n",
      " Background group: Zjets\n",
      " Sample: Zee\n",
      " Sample: Zmumu\n",
      " Sample: Ztautau\n",
      "Saved: Zjets_0ggF_full.json\n",
      "\n",
      " Background group: Wjets\n",
      " Sample: Wplusenu\n",
      " Sample: Wminusenu\n",
      " Sample: Wplusmunu\n",
      " Sample: Wminusmunu\n",
      " Sample: Wplustaunu\n",
      " Sample: Wminustaunu\n",
      "Saved: Wjets_0ggF_full.json\n",
      "\n",
      " Background group: diboson_leptonic\n",
      " Sample: llll\n",
      " Sample: lllv\n",
      " Sample: llvv\n",
      " Sample: lvvv\n",
      "Saved: diboson_leptonic_0ggF_full.json\n",
      "\n",
      " Background group: top\n",
      " Sample: ttbar_lep\n",
      " Sample: single_top_tchan\n",
      " Sample: single_antitop_tchan\n",
      " Sample: single_top_wtchan\n",
      " Sample: single_antitop_wtchan\n",
      " Sample: single_top_schan\n",
      " Sample: single_antitop_schan\n",
      " Sample: ttW\n",
      " Sample: ttee\n",
      " Sample: ttmumu\n",
      "Saved: top_0ggF_full.json\n",
      "\n",
      " Background group: HWW\n",
      " Sample: VBFH125_WW2lep\n",
      " Sample: ggH125_WW2lep\n",
      "Saved: HWW_0ggF_full.json\n",
      "\n",
      " Background group: diboson_semileptonic\n",
      " Sample: ZqqZll\n",
      " Sample: WqqZll\n",
      " Sample: WpqqWmlv\n",
      " Sample: WplvWmqq\n",
      " Sample: WlvZqq\n",
      "Saved: diboson_semileptonic_0ggF_full.json\n"
     ]
    }
   ],
   "source": [
    "# adjust these to go over different samples or different selections/no selections at all\n",
    "\n",
    "#channels = ['0ggF', '1ggF', 'VBF', 'VBF_new'] # OR VBF_new \n",
    "channels = ['0ggF']\n",
    "#channels = ['VBF_new']\n",
    "#background_categories = [\"Zjets\", \"Wjets\", \"diboson_leptonic\", \"top\"] # MC samples\n",
    "background_categories = [\"Zjets\", \"Wjets\", \"diboson_leptonic\", \"top\", \"HWW\", \"diboson_semileptonic\"] # MC samples\n",
    "#background_categories = ['top']\n",
    "#background_categories = ['HWW']\n",
    "#background_categories = [\"HWW\", \"diboson_semileptonic\"]\n",
    "selection_mode = \"full\"  # options: \"nocuts\", \"preselection\", \"full\"\n",
    "region_type = \"signal\" \n",
    "output_dir = \"background_json_by_selection_fixed_preselection_jets\" # where files are saved\n",
    "\n",
    "\n",
    "max_events = 50000 # max events\n",
    "lumi = 10  # fb^-1\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# start the loop\n",
    "for channel in channels:\n",
    "    print(f\"\\n Channel: {channel}\") # channels are the three different signal regions\n",
    "\n",
    "    for bkg_group in background_categories: # loop through backgrounds\n",
    "        print(f\"\\n Background group: {bkg_group}\")\n",
    "        grouped_data = {} # initialize\n",
    "\n",
    "        # my kernel keeps crashing for these specific backgrounds... decrease the step size\n",
    "        step_size = 250 if bkg_group in [\"Wjets\", \"diboson_leptonic\", \"diboson_semileptonic\", \"HWW\"] else 1000\n",
    "\n",
    "        # proper weight procedure (following example notebook from class)\n",
    "        for sample in samples[bkg_group][\"list\"]:\n",
    "            print(f\" Sample: {sample}\")\n",
    "            dsid = infofile.infos[sample][\"DSID\"]\n",
    "            xsec = infofile.infos[sample][\"xsec\"]\n",
    "            sumw = infofile.infos[sample][\"sumw\"]\n",
    "            red_eff = infofile.infos[sample][\"red_eff\"]\n",
    "\n",
    "            # now accessing the file\n",
    "            file_path = f\"{path}{skim_2lep}/MC/mc_{dsid}.{sample}.{skim_2lep}.root\"\n",
    "            tree = uproot_open(file_path + \":mini\")\n",
    "            total_events = tree.num_entries\n",
    "            # scale depending on how many events are being used\n",
    "            sample_scale = total_events / min(total_events, max_events) # however many events are used\n",
    "            xsec_weight = sample_scale * (lumi * 1000 * xsec) / (sumw * red_eff) # get the weight\n",
    "\n",
    "            sample_events = [] # initialize for loop\n",
    "\n",
    "            # weight and data\n",
    "            for data_w, data in zip(\n",
    "                tree.iterate(weight_vars, library=\"pd\", step_size=step_size, entry_stop=max_events),\n",
    "                tree.iterate(variables, library=\"ak\", step_size=step_size, entry_stop=max_events)\n",
    "            ):\n",
    "\n",
    "                # derived quantities using functions\n",
    "                data[\"dilepton_phi\"] = calc_phi_dilepton(data[\"lep_pt\"], data[\"lep_eta\"], data[\"lep_phi\"], data[\"lep_E\"]) # phi for dilepton system\n",
    "                data[\"delta_phi_ll_met\"] = delta_phi(data[\"dilepton_phi\"], data[\"met_phi\"]) # angle between dilepton system and MET\n",
    "                data[\"osof_mass\"] = calc_mass(data[\"lep_pt\"], data[\"lep_eta\"], data[\"lep_phi\"], data[\"lep_E\"]) # dilepton invariant mass\n",
    "                data[\"mT\"] = calc_mT(data[\"met_et\"], data[\"met_phi\"], data[\"lep_pt\"], data[\"lep_phi\"], data[\"osof_mass\"]) # dilepton transverse mass\n",
    "                data[\"mT_ell\"] = calc_mt_ell(data[\"lep_pt\"], data[\"lep_phi\"], data[\"met_et\"], data[\"met_phi\"]) # MAX transverse mass for lepton\n",
    "                data[\"pT_ll\"] = compute_pT_ll(data[\"lep_pt\"], data[\"lep_phi\"]) # dilepton transverse momentum\n",
    "                data[\"delta_phi_ll\"] = delta_phi(data[\"lep_phi\"][:, 0], data[\"lep_phi\"][:, 1]) # angle between the two leptons\n",
    "\n",
    "                #if only making preselections\n",
    "                # if selection_mode == \"preselection\": # if only choosing preselection cuts\n",
    "                #     data = apply_preselection(data, channel)\n",
    "                #     # move on if empty\n",
    "                #     if len(data) == 0:\n",
    "                #         continue\n",
    "                #     # weights\n",
    "                #     data_w = data_w.iloc[:len(data)] # not correct here\n",
    "\n",
    "                # if applying full SR selections\n",
    "                if selection_mode == \"full\": # if doing full SR cuts\n",
    "                    data, preselection_mask = apply_preselection(data, channel)\n",
    "                    #print(f\"After preselection: {len(data)} events\")\n",
    "                    # move on if empty\n",
    "                    if len(data) == 0:\n",
    "                        continue\n",
    "                    # weights\n",
    "                    # incorrect:\n",
    "                    #data_w = data_w.iloc[:len(data)] # NOT INDEXING PROPERLY\n",
    "                    # issue with mc weights here, I was just matching the lengths of these, not the proper indices\n",
    "                    data_w = data_w.iloc[ak.to_numpy(preselection_mask)] # properly apply mask to weights\n",
    "            \n",
    "\n",
    "                    # if using the new VBF SR\n",
    "                    if channel == 'VBF_new':\n",
    "                        region_mask = signal_region_VBF_new(data, channel) if region_type == \"signal\" else ak.Array([False] * len(data))\n",
    "                        data = data[region_mask] # apply mask\n",
    "                        data_w = data_w.iloc[ak.to_numpy(region_mask)] # weights\n",
    "                        if len(data) == 0:\n",
    "                            continue\n",
    "                    else:\n",
    "                        # otherwise use other specified signal region\n",
    "                        region_mask = signal_region(data, channel) if region_type == \"signal\" else ak.Array([False] * len(data))\n",
    "                        data = data[region_mask]\n",
    "                        data_w = data_w.iloc[ak.to_numpy(region_mask)]\n",
    "                        if len(data) == 0:\n",
    "                            continue\n",
    "\n",
    "                # compute mjj if looking at VBF\n",
    "                if channel == \"VBF\":\n",
    "                    data[\"mjj\"] = calc_mjj(data[\"jet_pt\"], data[\"jet_eta\"], data[\"jet_phi\"], data[\"jet_E\"]) # dijet invariant mass\n",
    "\n",
    "                # compute mjj and delta_y_jj for new VBF SR and apply cuts\n",
    "                if channel == \"VBF_new\":\n",
    "                    data[\"mjj\"] = calc_mjj(data[\"jet_pt\"], data[\"jet_eta\"], data[\"jet_phi\"], data[\"jet_E\"]) # dijet invariant mass\n",
    "                    #print(data['mjj'])\n",
    "                    #data[\"delta_y_jj\"] = delta_y_jj(data[\"jet_pt\"], data[\"jet_eta\"], data[\"jet_phi\"], data[\"jet_E\"])\n",
    "                    data[\"delta_y_jj\"] = ak.Array([\n",
    "                                                    delta_y_jj({\n",
    "                                                        \"jet_pt\": data[\"jet_pt\"][i], # jet pT\n",
    "                                                        \"jet_eta\": data[\"jet_eta\"][i], # jet eta\n",
    "                                                        \"jet_E\": data[\"jet_E\"][i] # jet energy\n",
    "                                                    }) if len(data[\"jet_pt\"][i]) >= 2 else np.nan\n",
    "                                                    for i in range(len(data))\n",
    "                                                ]) # difference in rapidity between the two jets\n",
    "                    #print(data['delta_y_jj'])\n",
    "                    #mjj_cut = (data['mjj'] > 400000) # apply mjj > 400 GeV\n",
    "                    mjj_cut = (data['mjj'] > 350000) # apply mjj > 350 GeV\n",
    "                    #print('mjj cut', mjj_cut)\n",
    "                    delta_y_jj_cut = np.abs(data['delta_y_jj'] > 2) # abs delta_y_jj > 1.5\n",
    "                    #print('dYjj cut', delta_y_jj_cut)\n",
    "                    combined_cut = mjj_cut & delta_y_jj_cut # mask combining these selections\n",
    "                    data = data[combined_cut] # apply the cut\n",
    "                    data_w = data_w.iloc[ak.to_numpy(combined_cut)] # weights\n",
    "                    if len(data) == 0: # if empty move on\n",
    "                        continue\n",
    "                    \n",
    "\n",
    "                # for events with jets\n",
    "                if \"jet_pt\" in data.fields:\n",
    "                    jets_over_30 = data[\"jet_pt\"] > 30000 # count number of jets with pT > 30 GeV\n",
    "                    data[\"Njets\"] = ak.sum(jets_over_30, axis=1)\n",
    "                else: # if jets not in the file\n",
    "                    data[\"Njets\"] = ak.zeros_like(data[\"lep_pt\"][:, 0], dtype=int)\n",
    "\n",
    "                # total weight\n",
    "                total_weight = data_w[weight_vars].prod(axis=1) * xsec_weight\n",
    "\n",
    "                # now store variables that we computed or were in the file to begin with\n",
    "                for i in range(len(data)):\n",
    "                    event = {\n",
    "                        \"delta_phi_ll_met\": float(data[\"delta_phi_ll_met\"][i]), # difference in angle between dilepton system and MET\n",
    "                        \"delta_phi_ll\": float(data[\"delta_phi_ll\"][i]), # angle between leptons\n",
    "                        \"mT\": float(data[\"mT\"][i]), # dilepton transverse mass\n",
    "                        \"mT_ell\": float(data[\"mT_ell\"][i]), # max of lepton transverse mass\n",
    "                        \"met_et\": float(data[\"met_et\"][i]), # MET\n",
    "                        \"met_phi\": float(data[\"met_phi\"][i]), # MET phi\n",
    "                        \"osof_mass\": float(data[\"osof_mass\"][i]), # dilepton invariant mass\n",
    "                        \"lep_pt\": [float(x) for x in data[\"lep_pt\"][i]], # lepton pT\n",
    "                        \"lep_eta\": [float(x) for x in data[\"lep_eta\"][i]], # lepton eta\n",
    "                        \"lep_phi\": [float(x) for x in data[\"lep_phi\"][i]], # lepton phi\n",
    "                        \"lep_E\": [float(x) for x in data[\"lep_E\"][i]], # lepton energy\n",
    "                        \"weight\": float(total_weight.iloc[i]), # weights\n",
    "                        \"Njets\": int(data[\"Njets\"][i]), # number of jets\n",
    "                        \"pT_ll\": float(data[\"pT_ll\"][i]), # dilepton transverse momentum\n",
    "                        \"dilepton_phi\": float(data[\"dilepton_phi\"][i]) # phi of dilepton system\n",
    "                    }\n",
    "                    # for VBF signal regions (involving jet variables)\n",
    "                    if channel in (\"VBF\", \"VBF_new\") and \"jet_pt\" in data.fields:\n",
    "                        jets = {\n",
    "                            \"jet_pt\": [float(x) for x in data[\"jet_pt\"][i]], # transverse momentum\n",
    "                            \"jet_eta\": [float(x) for x in data[\"jet_eta\"][i]], # jet eta\n",
    "                            \"jet_phi\": [float(x) for x in data[\"jet_phi\"][i]], # azimuthal angle\n",
    "                            \"jet_E\": [float(x) for x in data[\"jet_E\"][i]], # jet energy\n",
    "                            \"mjj\": float(data[\"mjj\"][i]) # dijet invariant mass\n",
    "                        }\n",
    "                        event.update(jets)\n",
    "\n",
    "                    # append that event with the variables saved\n",
    "                    sample_events.append(event)\n",
    "\n",
    "            grouped_data[sample] = sample_events\n",
    "            del tree, data, data_w, sample_events # delete for memory help\n",
    "            gc.collect()\n",
    "\n",
    "        # save to json file for the group\n",
    "        save_name = f\"{bkg_group}_{channel}_{selection_mode}.json\"\n",
    "        with open(os.path.join(output_dir, save_name), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"samples\": grouped_data, # save the samples\n",
    "                \"color\": samples[bkg_group][\"color\"], # colour it has\n",
    "                \"legend\": samples[bkg_group][\"legend\"] # title\n",
    "            }, f, indent=2)\n",
    "        print(f\"Saved: {save_name}\")\n",
    "        del grouped_data # for memory issues\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952321e0-e169-41b2-a5d7-490b0e14f199",
   "metadata": {},
   "source": [
    "# running on data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60c0cfbe-d945-4a3c-8b8a-04e8f8cd9154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " channel: 0ggF\n",
      "Data Sample: data_A\n",
      "Data Sample: data_B\n",
      "Data Sample: data_C\n",
      "Data Sample: data_D\n",
      "Saved: data_json_by_selection_test_FULL_0ggF_new\\data_0ggF_full.json\n"
     ]
    }
   ],
   "source": [
    "#channels = [\"0ggF\", \"1ggF\"]#, \"VBF\", \"VBF_new\"]\n",
    "#channels = ['1ggF']\n",
    "channels = ['0ggF']\n",
    "samples_data = {\n",
    "    'data': {\n",
    "        'list': ['data_A', 'data_B', 'data_C', 'data_D']\n",
    "    }\n",
    "}\n",
    "selection_mode = \"full\"  #  \"preselection\", \"full\", \"nocuts\"\n",
    "region_type = \"signal\"\n",
    "output_dir = \"data_json_by_selection_test_FULL_0ggF_new\" # where the json files are saved\n",
    "max_events = 6000000\n",
    "step_size = 600000\n",
    "os.makedirs(output_dir, exist_ok=True) # will make the directory if it doesn't exist\n",
    "\n",
    "for channel in channels: # loop through channels\n",
    "    print(f\"\\n channel: {channel}\")\n",
    "    period_data = {\"A\": [], \"B\": [], \"C\": [], \"D\": []} # all data files\n",
    "\n",
    "    for sample in samples_data[\"data\"][\"list\"]: # go through the files\n",
    "        print(f\"Data Sample: {sample}\")\n",
    "        period = sample.split(\"_\")[-1]\n",
    "        assert period in period_data\n",
    "        # get file path\n",
    "        file_path = path + skim_2lep + \"/Data/\" + sample + \".\" + skim_2lep + \".root\"\n",
    "        tree = uproot_open(file_path + \":mini\") # open the file using uproot\n",
    "\n",
    "        # go through the data\n",
    "        for data in tree.iterate(variables, library=\"ak\", step_size=step_size, entry_stop=max_events):\n",
    "            data[\"dilepton_phi\"] = calc_phi_dilepton(data[\"lep_pt\"], data[\"lep_eta\"], data[\"lep_phi\"], data[\"lep_E\"]) # phi of the two leptons\n",
    "            data[\"delta_phi_ll_met\"] = delta_phi(data[\"dilepton_phi\"], data[\"met_phi\"]) # difference in angle between the dilepton system and MET\n",
    "            data[\"osof_mass\"] = calc_mass(data[\"lep_pt\"], data[\"lep_eta\"], data[\"lep_phi\"], data[\"lep_E\"]) # dilepton invariant mass\n",
    "            data[\"mT\"] = calc_mT(data[\"met_et\"], data[\"met_phi\"], data[\"lep_pt\"], data[\"lep_phi\"], data[\"osof_mass\"]) # dilepton transverse mass\n",
    "            data[\"mT_ell\"] = calc_mt_ell(data[\"lep_pt\"], data[\"lep_phi\"], data[\"met_et\"], data[\"met_phi\"]) # max of lepton transverse mass\n",
    "            data[\"pT_ll\"] = compute_pT_ll(data[\"lep_pt\"], data[\"lep_phi\"]) # dilepton transverse momentum\n",
    "            data[\"delta_phi_ll\"] = delta_phi(data[\"lep_phi\"][:, 0], data[\"lep_phi\"][:, 1]) # difference in phi between the two leptons\n",
    "\n",
    "        # apply preselection if using preselection or full\n",
    "            if selection_mode in [\"preselection\", \"full\"]:\n",
    "                data = apply_preselection(data, channel)\n",
    "                if len(data) == 0: # skip if no events remain\n",
    "                    continue\n",
    "\n",
    "        # if applying full SR\n",
    "            if selection_mode == \"full\":\n",
    "                if channel == \"VBF_new\": # check case for VBF new (added)\n",
    "                    region_mask = signal_region_VBF_new(data, channel) if region_type == \"signal\" else ak.Array([False] * len(data))\n",
    "                    data = data[region_mask] # apply mask\n",
    "                    if len(data) == 0: # skip if empty\n",
    "                        continue\n",
    "                    # compute mjj and delta_y_jj\n",
    "                    data[\"mjj\"] = calc_mjj(data[\"jet_pt\"], data[\"jet_eta\"], data[\"jet_phi\"], data[\"jet_E\"])\n",
    "                    data[\"delta_y_jj\"] = ak.Array([\n",
    "                        delta_y_jj({\n",
    "                            \"jet_pt\": data[\"jet_pt\"][i], # jet pT\n",
    "                            \"jet_eta\": data[\"jet_eta\"][i], # jet eta\n",
    "                            \"jet_E\": data[\"jet_E\"][i] # jet energy\n",
    "                        }) if len(data[\"jet_pt\"][i]) >= 2 else np.nan\n",
    "                        for i in range(len(data))\n",
    "                    ])\n",
    "                    # apply cut for mjj > 400 GeV and abs(delta_y)jj) > 2\n",
    "                    mjj_cut = (data['mjj'] > 350000) # lower to 350 GeV\n",
    "                    delta_y_jj_cut = np.abs(data['delta_y_jj'] > 2)\n",
    "                    combined_cut = mjj_cut & delta_y_jj_cut\n",
    "                    data = data[combined_cut] # mask for events passing\n",
    "                    if len(data) == 0:\n",
    "                        continue\n",
    "                else: # if not new VBF SR, use the selected SR\n",
    "                    region_mask = signal_region(data, channel) if region_type == \"signal\" else ak.Array([False] * len(data))\n",
    "                    data = data[region_mask] # apply SR mask\n",
    "                    if len(data) == 0:\n",
    "                        continue\n",
    "\n",
    "            if channel == \"VBF\":  # look at mjj if using VBF\n",
    "                data[\"mjj\"] = calc_mjj(data[\"jet_pt\"], data[\"jet_eta\"], data[\"jet_phi\"], data[\"jet_E\"])\n",
    "           \n",
    "            # check number of jets with pT > 30 GeV for events with jets\n",
    "            if \"jet_pt\" in data.fields:\n",
    "                jets_over_30 = data[\"jet_pt\"] > 30000\n",
    "                data[\"Njets\"] = ak.sum(jets_over_30, axis=1)\n",
    "            else: # return 0 if not\n",
    "                data[\"Njets\"] = ak.zeros_like(data[\"lep_pt\"][:, 0], dtype=int)\n",
    "            # now store the calculated variables and the variables from the file\n",
    "            for i in range(len(data)):\n",
    "                event = {\n",
    "                    \"delta_phi_ll_met\": float(data[\"delta_phi_ll_met\"][i]), # angle between dilepton system and MET\n",
    "                    \"delta_phi_ll\": float(data[\"delta_phi_ll\"][i]), # angle between the two leptons\n",
    "                    \"mT\": float(data[\"mT\"][i]), # dilepton transverse mass\n",
    "                    \"mT_ell\": float(data[\"mT_ell\"][i]), # max of lepton transverse mass\n",
    "                    \"met_et\": float(data[\"met_et\"][i]), # MET\n",
    "                    \"met_phi\": float(data[\"met_phi\"][i]), # met phi\n",
    "                    \"osof_mass\": float(data[\"osof_mass\"][i]), # dilepton invariant mass\n",
    "                    \"lep_pt\": [float(x) for x in data[\"lep_pt\"][i]], # lepton pT\n",
    "                    \"lep_eta\": [float(x) for x in data[\"lep_eta\"][i]], # lepton eta\n",
    "                    \"lep_phi\": [float(x) for x in data[\"lep_phi\"][i]], # lepton phi\n",
    "                    \"lep_E\": [float(x) for x in data[\"lep_E\"][i]], # lepton energy\n",
    "                    \"Njets\": int(data[\"Njets\"][i]), # number of jets\n",
    "                    \"pT_ll\": float(data[\"pT_ll\"][i]), # dilepton transverse momentum\n",
    "                    \"dilepton_phi\": float(data[\"dilepton_phi\"][i]) # phi of dilepton system\n",
    "                }\n",
    "                # if looking at VBF SRs, store the jet variables\n",
    "                if channel in (\"VBF\", \"VBF_new\") and \"jet_pt\" in data.fields:\n",
    "                    jets = {\n",
    "                        \"jet_pt\": [float(x) for x in data[\"jet_pt\"][i]], # jet pT\n",
    "                        \"jet_eta\": [float(x) for x in data[\"jet_eta\"][i]], # jet energy\n",
    "                        \"jet_phi\": [float(x) for x in data[\"jet_phi\"][i]], # jet phi\n",
    "                        \"jet_E\": [float(x) for x in data[\"jet_E\"][i]], # jet energy\n",
    "                        \"mjj\": float(data[\"mjj\"][i]) # dijet invariant mass\n",
    "                    }\n",
    "                    event.update(jets)\n",
    "\n",
    "                # store for the period\n",
    "                period_data[period].append(event)\n",
    "\n",
    "        del tree, data # delete for memory issues\n",
    "        gc.collect()\n",
    "\n",
    "    # save output to a json file\n",
    "    save_path = os.path.join(output_dir, f\"data_{channel}_{selection_mode}.json\")\n",
    "    with open(save_path, \"w\") as f: # open a new file and save to it\n",
    "        json.dump({\"samples\": period_data}, f, indent=2)\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45aab1ec-0635-411c-9d85-2a4d9b04bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 0ggF\n",
      " - data_A (A): 668152 events\n",
      " - data_B (B): 2459370 events\n",
      " - data_C (C): 3587872 events\n",
      " - data_D (D): 5490396 events\n"
     ]
    }
   ],
   "source": [
    "# to check how many events are in the data files\n",
    "channels = [\"0ggF\"]\n",
    "# data A to D\n",
    "samples_data = {\n",
    "    'data': {\n",
    "        'list': ['data_A', 'data_B', 'data_C', 'data_D']\n",
    "    }\n",
    "}\n",
    "skim_2lep = \"2lep\" # using 2 lepton skim\n",
    "path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/\" # path for the files\n",
    "for channel in channels: # loop through\n",
    "    print(f\"channel {channel}\")\n",
    "    for sample in samples_data[\"data\"][\"list\"]:\n",
    "        period = sample.split(\"_\")[-1]\n",
    "        file_path = path + skim_2lep + \"/Data/\" + sample + \".\" + skim_2lep + \".root\" # get the file path\n",
    "        try: # try to open if proper file path\n",
    "            tree = uproot_open(file_path + \":mini\")\n",
    "            print(f\" - {sample} ({period}): {tree.num_entries} events\")\n",
    "        except Exception as e: # if path is messed up\n",
    "            print(f\"file issue/wrong file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8cc7b-d3b8-41b8-887d-374ce068482d",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "weight info from sample notebook:\n",
    "\n",
    "\n",
    "Additionally,\n",
    "    there is a cross-section weight $w_\\sigma$ associated with each MC file.\n",
    "We define this variable `xsec_weight` below. \n",
    "This weight is meant to normalise the entire Monte-Carlo distribution based on the number of events in the data.\n",
    "This is its definition:\n",
    "$$ w_\\sigma = \\frac{\\int L \\text{d}t ~ \\sigma }{\\eta \\sum_i w_i } $$\n",
    "where $\\int L \\text{d}t$ is the integrated luminosity (`lumi`),\n",
    "    $\\sigma$ is the cross section (`info[\"xsec\"]`),\n",
    "    $\\eta$ is the filter efficiency of the MC generator,\n",
    "    and $\\sum_i w_i$ gives the sum of all weights (`info[\"sumw\"]`).\n",
    "When the integrated luminosity is multiplied by the cross section,\n",
    "    it gives a measure of the total number of events during a period of data taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c990f6-2749-44f5-9fcc-bbf958997e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, True], [True, True, True]]\n"
     ]
    }
   ],
   "source": [
    "# testing the overlap removal\n",
    "# testing the overlap function first:\n",
    "# sample arrays\n",
    "jet_eta = ak.Array([[0.5, -1.1], [1.2, -2.0, 0.3]])\n",
    "jet_phi = ak.Array([[0.1, -0.1], [1.0, -1.5, 0.2]])\n",
    "lep_eta = ak.Array([[1.1, -0.3], [0.4, -1.7]])\n",
    "lep_phi = ak.Array([[0.0, 0.3], [-1.2, 0.8]])\n",
    "\n",
    "# check overlap removal\n",
    "keep = remove_jets_near_leptons(jet_eta, jet_phi, lep_eta, lep_phi, deltaR=0.2)\n",
    "print(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d22cb953-909b-4fbd-8c9c-556cdffb3f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5, -1.1], [1.2, -2, 0.3]]\n"
     ]
    }
   ],
   "source": [
    "# check mask\n",
    "filtered_pt = jet_eta[keep]\n",
    "print(filtered_pt)\n",
    "# keeps all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a7ec66e-9981-490b-9f38-69778915841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply filter\n",
      "\n",
      "Event 0\n",
      "jet_pt: [40000]\n",
      "jet_eta: [0.5]\n",
      "jet_phi: [0.1]\n",
      "jet_E: [50000]\n",
      "jet_MV2c10: [0.1]\n",
      "\n",
      "Event 1\n",
      "jet_pt: [60000, 50000, 30000]\n",
      "jet_eta: [1.2, -2, 0.3]\n",
      "jet_phi: [1, -1.5, 0.2]\n",
      "jet_E: [70000, 60000, 40000]\n",
      "jet_MV2c10: [0.3, 0.85, 0.4]\n"
     ]
    }
   ],
   "source": [
    "# Sample data: two events\n",
    "data = {\n",
    "    \"jet_pt\": ak.Array([[40_000, 20_000], [60_000, 50_000, 30_000]]),  # 2 jets in event 0, 3 jets in event 1\n",
    "    \"jet_eta\": ak.Array([[0.5, -0.8], [1.2, -2.0, 0.3]]),\n",
    "    \"jet_phi\": ak.Array([[0.1, -0.1], [1.0, -1.5, 0.2]]),\n",
    "    \"jet_E\": ak.Array([[50_000, 30_000], [70_000, 60_000, 40_000]]),\n",
    "    \"jet_MV2c10\": ak.Array([[0.1, 0.2], [0.3, 0.85, 0.4]])\n",
    "}\n",
    "\n",
    "# sample mask - want to remove 2nd jet in the first event, keep all jets in the second event\n",
    "keep_mask = ak.Array([[True, False], [True, True, True]])\n",
    "\n",
    "# check condition\n",
    "if not ak.all(keep_mask):\n",
    "    print(\"apply filter\")\n",
    "    jet_keys = [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_E\", \"jet_MV2c10\"]\n",
    "    data.update({key: data[key][keep_mask] for key in jet_keys})\n",
    "\n",
    "# check output\n",
    "for i in range(len(data[\"jet_pt\"])):\n",
    "    print(f\"\\nEvent {i}\")\n",
    "    for key in data:\n",
    "        print(f\"{key}: {data[key][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd81f32-ebff-4f05-bcd5-66504d12002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed the second jet in the first event and kept all jets in the second event"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
